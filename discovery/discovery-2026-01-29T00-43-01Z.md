# Discovery: 2026-01-29T00-43-01Z

## User Vision & Insights

### Vision Statement
Improve the AI Coding Template's context retrieval system to get the RIGHT context efficiently - not dump everything into prompts. Focus on token-efficient, accurate context gathering using Archon's RAG knowledge base and code examples.

### Key Challenges
- **Prime command inefficiency**: Currently dumps entire codebase, token wasteful
- **Mixed-language RAG results**: Queries may return irrelevant language examples (Python results when working on TypeScript)
- **No smart filtering**: System doesn't adapt context based on project type or task requirements
- **Session context loss**: After `/clear`, must re-prime everything even if most files unchanged

### Ideas & Requests
1. **Contextual Embedding** - Enable Anthropic's contextual retrieval in Archon for better accuracy
2. **Soft Smart Filtering** - Boost relevant languages based on project type, don't hard exclude
3. **Diff-Based Prime** - Only detail changed files, summarize unchanged
4. **Token Monitoring Agent** - Future: Detect where tokens are wasted (later iteration)

### Success Criteria
| Metric | Priority | Acceptable |
|--------|----------|------------|
| Retrieval Accuracy | #1 Critical | Right context returned |
| Token Savings | #2 Important | Measurable reduction |
| Speed | #3 Nice-to-have | 2-3 min if accurate |

### Key Terminology
- "Right context, not all context"
- "Token efficient"
- "Language-aware filtering"
- "Contextual embedding"

---

## Codebase Overview

### Project Structure
```
AI Coding Template/
├── .claude/commands/    # 12 workflow commands
├── discovery/           # Discovery documents
├── AGENT.md            # Universal AI instructions
├── CLAUDE.md           # Claude Code specific config
├── INDEX.md            # Navigation
└── README.md           # Quick start
```

### Technology Stack
- **Platform**: Multi-CLI (Claude Code, Codex, Cursor compatible)
- **Task Management**: Archon MCP (primary)
- **Knowledge Base**: Archon RAG with 6 sources (~1.3M words)
- **Embedding**: Text embedding + optional contextual embedding
- **Reranking**: Qwen 3 Coder

### Patterns and Conventions
- Markdown-based commands in `.claude/commands/`
- Archon-first task management (no TodoWrite)
- RAG knowledge base for code examples and documentation
- Workflow phases: Discovery → Planning → Development → Execution → Review → Test

---

## Ideas for AI Agents / Features

### High Priority

#### 1. Contextual Embedding Integration
- **Description**: Enable Anthropic's contextual retrieval in Archon
- **Impact**: 35-67% improvement in retrieval accuracy
- **Implementation**: Generate context for each chunk during ingestion using Qwen 3 Coder
- **Reference**: [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

#### 2. Soft Smart Language Filtering
- **Description**: Auto-detect project language and boost (not exclude) relevant sources
- **Impact**: More relevant results without losing cross-language patterns
- **Implementation**:
  - Detect: requirements.txt → Python, package.json → TypeScript
  - Boost: Python sources 1.5x for Python projects
  - Keep: General sources (MCP, Anthropic docs) at 1.0x
  - Reduce: Non-matching languages at 0.7x (still included)

#### 3. Diff-Based Prime Command
- **Description**: Optimize `/prime` to only detail changed files
- **Impact**: 70-90% token savings on subsequent primes
- **Implementation**:
  - Store file hashes in `.prime-cache.json`
  - Compare on each prime
  - Output: Full detail for changed, summary for unchanged

### Medium Priority

#### 4. Semantic Context Cache
- **Description**: Cache embeddings of codebase, query relevant parts only
- **Impact**: Significant token savings, faster context retrieval
- **Complexity**: High - requires embedding storage and similarity search

### Low Priority (Future)

#### 5. Token Usage Monitoring Agent
- **Description**: AI agent that monitors where tokens are wasted and context is lost
- **Impact**: Continuous optimization insights
- **Complexity**: High - requires usage tracking and analysis

---

## Inspiration Sources

### Documentation
- [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) - Core technique for improving RAG accuracy by 35-67%
- [Together.ai Implementation Guide](https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic) - Practical implementation steps
- [DataCamp Tutorial](https://www.datacamp.com/tutorial/contextual-retrieval-anthropic) - Step-by-step tutorial

### Best Practices
- **Chunk boundaries matter**: Document segmentation strategy affects retrieval
- **Embedding selection**: Gemini and Voyage embeddings showed superior performance
- **Hybrid search**: Combine semantic (dense) + keyword (BM25) for best results
- **Reranking**: Add reranking step for additional 18% improvement

### Archon Knowledge Sources
| Source | Words | Use Case |
|--------|-------|----------|
| MCP Documentation | 200k | Protocol/SDK patterns |
| Supabase llms.txt | 508k | Database/Backend |
| PydanticAI | 444k | Python AI patterns |
| Supabase Edge | 70k | TypeScript/Edge |
| Anthropic Docs | 34k | Claude API |
| Ollama Docs | 28k | Local LLM |

---

## Needs Analysis

### Current Gaps
1. **No contextual embedding**: Chunks lack situational context, reducing retrieval accuracy
2. **No language awareness**: RAG queries return all languages regardless of project type
3. **Inefficient prime**: Full codebase dump every session wastes tokens
4. **No incremental context**: Can't resume session with just changes

### Improvement Opportunities
1. **Enable contextual embedding in Archon** - Configuration change + re-index
2. **Add language detection to RAG queries** - Middleware to detect and boost
3. **Implement diff-based prime** - Store hashes, compare, output diff

### Emerging Opportunities
1. **Prompt caching**: Reduce contextual embedding costs (~$1.02/million tokens)
2. **Hybrid BM25 + semantic**: Combine for 49% better retrieval
3. **Cross-session context persistence**: Remember project state between sessions

---

## Opportunities

| Opportunity | Impact | Feasibility | Priority | Effort |
|-------------|--------|-------------|----------|--------|
| Contextual Embedding in Archon | High | Medium | High | 2-3 days |
| Soft Smart Language Filtering | High | High | High | 1 day |
| Diff-Based Prime Command | High | High | High | 1 day |
| Semantic Context Cache | Medium | Low | Medium | 1 week |
| Token Monitoring Agent | Low | Low | Low | 2+ weeks |

---

## Recommended MVP Focus

### MVP: Smart Context Retrieval System

**Goal**: Improve retrieval accuracy while reducing token usage

**Key Features** (in priority order):
1. **Soft Smart Language Filtering** - Quick win, high impact
2. **Diff-Based Prime** - Significant token savings
3. **Contextual Embedding** - Best accuracy improvement (if Archon supports)

**Success Metrics**:
- Retrieval returns relevant language examples
- Token usage reduced by 50%+ on subsequent primes
- Accuracy improvement measurable via relevance of results

**Architecture**:
```
[Project Detection] → [Language Boost Weights] → [Archon RAG Query]
                                                        ↓
[Diff-Based Prime] → [Changed Files Detail] → [Context Output]
                   → [Unchanged Summary]
```

---

## Next Steps

1. **Immediate**: Create PRD for Smart Context Retrieval MVP
2. **Planning**: Define technical spec for language filtering + diff prime
3. **Research**: Check Archon settings for contextual embedding toggle
4. **Implementation**: Start with soft language filtering (quickest win)

---

## Timestamp
2026-01-29T00:43:01Z
